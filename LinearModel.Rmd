---
title: "LinearModel"
author: "Karen Hernandez"
date: "11/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
dataF <- read.csv(file="http://onlinestatbook.com/case_studies_rvls/physical_strength/data.txt",sep="",header=TRUE) 
```

```{r include=FALSE}
require(tidyverse)
require(tigerstats)
require(rgl)
require(knitr)
```


## Now with ggplot - first select the basic data

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=ARM))
```
## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
```

## Now with ggplot - first select the basic data

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=GRIP))
```
## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
```

The plot of SIMS versus Grip plus ARM look like a plane in a 3 dimensional space and we don't have the tools to plot it. 



```{r}
model.1 <- lm(SIMS~ARM,data=dataF)
summary.lm(model.1)
```

We can see that this model 1, the adjusted R squared is 0.467 which is about 47% variations more than the mean model did. So this one is explaining more of the variations. 

```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.1,new)
predict(model.1,new,interval="prediction")
```

Model 1 is predicting a value of 0.7063836 and the 95 confidence interval is somewhere in the -1.726209 and 3.138977.

```{r}
model.2 <- lm(SIMS~GRIP,data=dataF)
summary.lm(model.2)
``` 

We can see that model 2 the adjusted r squared is 0.4053


```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.2,new)
predict(model.2,new,interval="prediction")
```

Model 2 is predicting a value of -0.5361543 and the 95 confidence interval is somewhere in the -3.107961 and 2.035652

```{r}
model.3 <- lm(SIMS~GRIP+ARM,data=dataF)
summary.lm(model.3)
``` 

The adjusted R squared we can see that it is 0.5358 which is bigger adjusted R squared than model 1 and model 2. It explains more of the variation than either oof the other 2 model. This is the best model. Just by exploring the adjusted R squared if we are to rate the models Model 2 is the worst one. Model 1 is the second best one and Model 3 is the best model. 

```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.3,new)
predict(model.3,new,interval="prediction")
```

Model 3 is predicting a vaule of 0.1496476 and the 95 confidence interval is somewhere in the -2.132373 and 2.431668.

```{r}
anova(model.1,model.3)
```

The RSS of Model 1 is 217.88 and RSS of Model 2 is 188.43 which is about 30 points less than SIMS-ARM. This show that the result is significant becasue we are testing whether or not it would change the vaule of our assessment. The results shows a low P value so we reject the null hypothesis that there isn't any difference in how will the Model 1 explains ARM and how well Model 2 explains ARM. This means that model 3 is doing a much better job of predicting it and thats why we reject the null hypothesis. From this anova test we can conclude that model 3 is a much better model.

---
title: "LinearModel"
author: "Karen Hernandez"
date: "11/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
dataF <- read.csv(file="http://onlinestatbook.com/case_studies_rvls/physical_strength/data.txt",sep="",header=TRUE) 
```

```{r include=FALSE}
require(tidyverse)
require(tigerstats)
require(rgl)
require(knitr)
```


## Now with ggplot - first select the basic data

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=ARM))
```
## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
```

## Now with ggplot - first select the basic data

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=GRIP))
```
## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
```

The plot of SIMS versus Grip plus ARM look like a plane in a 3 dimensional space and we don't have the tools to plot it. 



```{r}
model.1 <- lm(SIMS~ARM,data=dataF)
summary.lm(model.1)
```

We can see that this model 1, the adjusted R squared is 0.467 which is about 47% variations more than the mean model did. So this one is explaining more of the variations. 

```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.1,new)
predict(model.1,new,interval="prediction")
```

```{r}
model.2 <- lm(SIMS~GRIP,data=dataF)
summary.lm(model.2)
``` 

We can see that model 2 the adjusted r squared is 0.4053


```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.2,new)
predict(model.2,new,interval="prediction")
```

```{r}
model.3 <- lm(SIMS~GRIP+ARM,data=dataF)
summary.lm(model.3)
``` 

The adjusted R squared we can see that it is 0.5358 which is bigger than model 1 and model 2. It explains more of the variation than either of both of them. This is the best model. 

```{r}
new <- data.frame(ARM=88, GRIP=94)
predict.lm(model.3,new)
predict(model.3,new,interval="prediction")
```

```{r}
anova(model.1,model.3)
```

The RSS of Model 1 is 217.88 and RSS of Model 2 is 188.43 which is about 30 points less than SIMS-ARM. This show that the result is significant becasue we are testing whether or not it would change the vaule of our assessment. The results shows a low P value so we reject the null hypothesis that there isn't any difference in how will the Model 1 explains ARM and how well Model 2 explains ARM. This means that model 3 is doing a much better job of predicting it and thats why we reject the null hypothesis. From this anova test we can conclude that model 3 is a much better model.
